from itertools import islicefrom transformers import T5Tokenizer, T5EncoderModel, AutoModelForSeq2SeqLMimport torchfrom AccessiontoAlignment import create_dictionary_from_alignment, dictionary_to_fastaimport sysDICT_SIZE = 10ALN = sys.argv[1]EMBED_FILE = sys.argv[2]def prost_embed_aln(aln_file, new_embed_file):    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')    tokenizer = T5Tokenizer.from_pretrained('Rostlab/ProstT5', do_lower_case=False)    model = T5EncoderModel.from_pretrained("Rostlab/ProstT5").to(device)    # only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)    model.full() if device == 'cpu' else model.half()    protein_sequences = create_dictionary_from_alignment(aln_file)    protein_sequences = {label: f"<AA2fold> {' '.join(list(seq.replace('-', 'X')))}" for label, seq in                         protein_sequences.items()}    def run_model(input_seq_dict):        ids = tokenizer.batch_encode_plus(input_seq_dict.values(),                                          add_special_tokens=True,                                          padding="longest",                                          padding_side='right',                                          return_tensors='pt').to(device)        with torch.no_grad():            embedding_rpr = model(                ids.input_ids,                attention_mask=ids.attention_mask            )        embed_dict = {id: tensor.squeeze(0) for id, tensor in                      zip(input_seq_dict.keys(), torch.split(embedding_rpr.last_hidden_state, 1, dim=0))}        return embed_dict    if len(protein_sequences)>DICT_SIZE:        seq_iter = iter(protein_sequences.items())        chopped_up_seq_dict = [dict(islice(seq_iter, DICT_SIZE)) for _ in                               range(0, len(protein_sequences.items()), DICT_SIZE)]        embed_dicts = [run_model(dictionary) for dictionary in chopped_up_seq_dict]        embed_dicts = {key: value for dictionary in embed_dicts for key, value in dictionary.items()}    else:        embed_dicts = run_model(protein_sequences)    torch.save(embed_dicts, new_embed_file)def get_3di(aln_file, threedi_seq_fasta):    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')    # Load the tokenizer    tokenizer = T5Tokenizer.from_pretrained('Rostlab/ProstT5', do_lower_case=False)    # Load the model    model = AutoModelForSeq2SeqLM.from_pretrained("Rostlab/ProstT5").to(device)    # only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)    model.full() if device == 'cpu' else model.half()    # prepare your protein sequences/structures as a list.    # while 3Di-sequences need to be lower-case.    protein_sequences = create_dictionary_from_alignment(aln_file)    max_len = max([len(s.replace('-', 'X')) for s in protein_sequences.values()])    protein_sequences = {label: f"<AA2fold> {' '.join(list(seq.replace('-', 'X')))}" for label, seq in                         protein_sequences.items()}    seq_iter = iter(protein_sequences.items())    chopped_up_seq_dict = [dict(islice(seq_iter, DICT_SIZE)) for _ in                           range(0, len(protein_sequences.items()), DICT_SIZE)]    def run_model(input_seq_dict):        # tokenize sequences and pad up to the longest sequence in the batch        ids = tokenizer.batch_encode_plus(input_seq_dict.values(),                                          add_special_tokens=True,                                          padding="longest",                                          max_length=max_len,                                          return_tensors='pt').to(device)        # Generation configuration for "folding" (AA-->3Di)        gen_kwargs_aa2fold = {            "do_sample": True,            "num_beams": 3,            "top_p": 0.95,            "temperature": 1.2,            "top_k": 6,            "repetition_penalty": 1.2,        }        with torch.no_grad():            translations = model.generate(                ids.input_ids,                attention_mask=ids.attention_mask,                max_length=max_len,  # max length of generated text                min_length=max_len,  # minimum length of the generated text                early_stopping=False,  # stop early if end-of-text token is generated                num_return_sequences=1,  # return only a single sequence                **gen_kwargs_aa2fold            )        # Decode and remove white-spaces between tokens        decoded_translations = tokenizer.batch_decode(translations, skip_special_tokens=True)        seq_3dis = {label: "".join(ts.split(" ")) for label, ts in                    zip(input_seq_dict.keys(), decoded_translations)}  # predicted 3Di strings        return seq_3dis    sed_3di_dicts = [run_model(dictionary) for dictionary in chopped_up_seq_dict]    structure_sequences = {key: value for dictionary in sed_3di_dicts for key, value in dictionary.items()}    dictionary_to_fasta(structure_sequences, threedi_seq_fasta)def embed_3di(fasta_3di, embed_file):    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')    tokenizer = T5Tokenizer.from_pretrained('Rostlab/ProstT5', do_lower_case=False)    model = T5EncoderModel.from_pretrained("Rostlab/ProstT5").to(device)    # only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)    model.full() if device == 'cpu' else model.half()    protein_sequences = create_dictionary_from_alignment(fasta_3di)    protein_sequences = {label: f"<fold2AA> {' '.join(list(seq.lower()))}" for label, seq in                         protein_sequences.items()}    max_len = max([len(s) for s in protein_sequences.values()])    seq_iter = iter(protein_sequences.items())    chopped_up_seq_dict = [dict(islice(seq_iter, DICT_SIZE)) for _ in                           range(0, len(protein_sequences.items()), DICT_SIZE)]    print(max_len)    def run_model(input_seq_dict):        ids = tokenizer.batch_encode_plus(input_seq_dict.values(),                                          add_special_tokens=True,                                          padding="longest",                                          max_length=max_len,                                          return_tensors='pt').to(device)        with torch.no_grad():            embedding_rpr = model(                ids.input_ids,                attention_mask=ids.attention_mask            )        embed_dict = {id: tensor.squeeze(0) for id, tensor in                      zip(input_seq_dict.keys(), torch.split(embedding_rpr.last_hidden_state, 1, dim=0))}        return embed_dict    embed_3di_dicts = [run_model(dictionary) for dictionary in chopped_up_seq_dict]    total_dict = {key: value for dictionary in embed_3di_dicts for key, value in dictionary.items()}    torch.save(total_dict, embed_file)if __name__ == '__main__':    prost_embed_aln(ALN, EMBED_FILE)
import math
import os, re, subprocess
import AccessiontoAlignment, ESM, ChimeraGenerator, Analysis, GromacsAnalysis
from json import load
from pathlib import Path
import numpy as np
import pandas as pd

"""
Contained below are all the arguments found in the chimera generation jsons that require deeper explanation outside their name. 
This is not an exhaustive list

#Fasta Arguments#
number_of_subunits: int: 'Number representing how many homo-monomeric proteins make up the quartenary structure of your protein'

sequence_of_interest: str: 'This is the path to the fasta containing the sequence of either the constant protein you want spliced into the 
variant proteins, or the section of the variants you want replaced with the constant protein. Each sequence of 
interest should be in their own fasta file. All non-reference variant proteins will have the Homomer sequence 
from an alignment replaced'

parent_msa: str
'Path to the multiple sequence alignment to be created or a user-generated msa of all parent sequences'

collective_fasta: str
'Path to the fasta file to be created containing all sequences to be run on Alphafold (parents+chimeras) and their 
inheritance dicts. Path must have .inh suffix'

fasta_toggles: dict
'A dictionary that holds optional operations within the fasta operation, they are generally turned on by 
placing True within the quotes of their value'

Make_pair_or_combo_heteromers: bool
'This toggle either creates every combination between all protein list generated by each json file if "combo" 
is entered in the json file, if "pair" is entered all chimera lists in the per json file will be paired one to 
one to one etc. or single file list will be created for homomeric proteins. Switch toggle to pair if you one of 
your unique proteins will not be a chimera'

#Submission Argument#

submission_toggles: dict
'A dictionary that holds optional operations within the submission operation, they are generally turned on by 
placing true within the quotes of their value'

create_slurms: bool :toggle
sbatch_slurms: bool :toggle
run_stragglers: bool :toggle
run_AF: bool :toggle
'Set to true to run AlphaFold'
run_gromacs: bool
'A custom_label_list is required in order to run gromacs. Set to true to run gromacs'
alphafold_settings: dict = {"-p": "gpu", "--gres": "gpu:h200:1", "-N": 1, "-n": 3, "-A": "enter_here",
                            "--mem": "400000", "--time": "72:00:00", "-J": "swinesars"},
'Sbatch settings for running alphafold in slurm systems'

sequence_identity_cutoff: float
'Sequence identity cutoff for chimeras made with homologous operations'

embedding_settings: dict = {"-p": "gpu", "--gres": "gpu:a100:1", "-N": 1, "-n": 3, "-A": "enter_here",
                            "--time": "72:00:00", "-J": "swinesars"}
'Sbatch settings for running prost in slurm systems'

gromacs_settings: dict ={"-p":"gpu", "--gres":"gpu:1", "-N":1, "-A":"", "--time":"72:00:00","-J": "","--ntasks-per-node":"10"}
'Sbatch settings for running gromacs in slurm systems'

gromacs_commands: list = ["srun gmx_mpi","module purge","module load gcc/11.4.0 openmpi/4.1.4 gromacs/2023.2"]
'list of commands to be executed in the commandline before calling gromacs. The first item in the list must be the command required to actually run gromacs.'

custom_label_list: str
'If a custom list is provided, only labels provided will be submitted to slurm. Path to text file that contains line 
separated protein labels as they appear in the multifasta/alignment you want predicted'

alphafold_shell_script: str: 
'Path to the shell script that runs alphafold, it must take a list of comma-separated fastas 
and an output as its arguments.'

embedding_shell_script: str: 
'Path to the shell script that runs prost, it must take a fasta with all sequences you want embedded and 
the filename for the new pkl file that will hold the embeddings.'

#Analysis Arguments#

analysis_toggles: dict
make_plddts: bool: toggle
make_pdbs: bool : toggle
make_emboss_files: bool

metadata: dict: 'A dictionary containing metadata, like different function settings you want attached to the csv/table outputs of analysis.
Date (M-D-YYYY) is added by default'

analysis_output_file: str:
'Path of the file you want created that will contain all analysis created.'

"""
ALPHAFOLD_FOLDERNAME = 'AlphaFold'
FASTA_FOLDERNAME = 'Fasta'
GMX_FOLDERNAME = 'Gromacs'
FASTA_EXT = '.fa'
INHERITANCE_EXT = '.inh'


def make_slurm_content(sbatch_flags: dict, commands: list, shebang='#!/bin/bash'):
    flags = dict(sbatch_flags)
    return '\n'.join(
        [shebang] + [f'#SBATCH {flag}={arg}' if flag.startswith('--') else f'#SBATCH {flag} {arg}' for flag, arg
                     in flags.items()] + [' && \n'.join(commands)])


def alphafold_submission(fastas_to_run, alphafold_shell_script: str, output_directory: str, slurm_settings: dict,
                         proteins_per_slurm=7, make_slurms=False):
    output_directory = Path(output_directory)
    #TODO so alpha_Dir is output instead of crteating a directory
    output_directory.mkdir(exist_ok=True)
    # TODO fix so this only does one at a time??
    for slurm_index, file_index in enumerate(range(0, len(fastas_to_run), proteins_per_slurm)):
        settings = dict(slurm_settings)
        fastas = ",".join(fastas_to_run[file_index:file_index + proteins_per_slurm])
        submit_args = ['sh', alphafold_shell_script, fastas, str(output_directory)]
        job_id = str(slurm_index) + settings.get('-J', 'slurm')
        settings['-J'] = job_id
        if not settings.get('-e', None):
            # TODO need to make it so Slurm is made as a sibling of output_directory
            settings['-e'] = output_directory.joinpath('Slurms', job_id).with_suffix(".err")
        input_slurm = make_slurm_content(settings, [' '.join(submit_args)])
        if make_slurms:
            output_directory.joinpath('Slurms').mkdir(exist_ok=True)
            new_slurm = output_directory.joinpath('Slurms', job_id).with_suffix('.slurm')
            new_slurm.write_text(input_slurm)
        result = subprocess.run(['sbatch'], text=True, input=input_slurm, capture_output=True)
        print(result.stdout, result.stderr)


def prost_submission(fasta_file_to_run, new_pkl_file, output_directory, prost_shell_script, slurm_settings: dict,
                     make_slurms=False):
    settings = dict(slurm_settings)
    submit_args = ['sh', prost_shell_script, fasta_file_to_run, new_pkl_file]
    input_slurm = make_slurm_content(settings, [' '.join(submit_args)])
    # TODO need to make it so Slurm is made as a sibling of output_directory
    if make_slurms:
        output_directory.joinpath('Slurms').mkdir(exist_ok=True)
        new_slurm = output_directory.joinpath('Slurms', slurm_settings.get('-J', 'embed_slurm')).with_suffix('.slurm')
        new_slurm.write_text(input_slurm)
    result = subprocess.run(['sbatch'], text=True, input=input_slurm, capture_output=True)
    print(result.stdout, result.stderr)


def gmx_submission(pdb, slurm_settings: dict, output_directory, working_directory=None, forcefield='charmm36-jul2022',
                   make_slurms=False, gmxbin='gmx_mpi',
                   additional_commandline: list = None, gromacs_production_only=False):
    settings = dict(slurm_settings)
    job_number=None
    if working_directory is None:
        working_directory = output_directory
    output_directory = Path(output_directory)
    shortname = output_directory.joinpath(Path(pdb).stem)
    job_id = settings.get('-J', shortname.stem)
    settings['-J'] = job_id
    settings['--chdir'] = output_directory.__str__() if working_directory is None else working_directory
    #export GMXLIB=(directory with custom forcefield)
    if not settings.get('-e', None):
        settings['-e'] = output_directory.joinpath(job_id).with_suffix(".err")
    if not settings.get('-o', None):
        settings['-o'] = output_directory.joinpath(job_id).with_suffix(".out")
    if not gromacs_production_only:
        # let you choose forcefield, maybe have symbolic ln or just put manually like mdp files
        commands = [
            f'{gmxbin} pdb2gmx -f {pdb} -o {shortname} -p {shortname} -i {shortname} -ff {forcefield} -water tip3p -ignh',
            f'{gmxbin} editconf -f {shortname}.gro -o {shortname}_box -bt o -c -d 1',
            f'{gmxbin} grompp -p {shortname} -c {shortname}_box -f minim.mdp -o {shortname}_em -maxwarn 1',
            f'{gmxbin} mdrun -v -deffnm {shortname}_em',
            f'{gmxbin} grompp -p {shortname} -c {shortname} -f minim.mdp -o {shortname}_em -maxwarn 1',
            f'{gmxbin} solvate -cp {shortname}_em -o {shortname}_sol -p {shortname}',
            f'{gmxbin} grompp -p {shortname} -c {shortname}_sol -f minim.mdp -o {shortname}_preion -maxwarn 1',
            f'echo 13 | {gmxbin} genion -s {shortname}_preion -o {shortname}_ions -p {shortname} -neutral -conc 0.15',
            f'{gmxbin} grompp -p {shortname}.top -c {shortname}_ions -f minim.mdp -o {shortname}_em -maxwarn 1',
            f'{gmxbin} mdrun -v -deffnm {shortname}_em',
            f'{gmxbin} grompp -p {shortname} -c {shortname}_em -f startup.mdp -o {shortname}_start -maxwarn 1',
            f'{gmxbin} mdrun -v -deffnm {shortname}_start',
            f'{gmxbin} grompp -p {shortname} -c {shortname}_start -f prod.mdp -o {shortname}_prod -maxwarn 1']
        input_slurm = make_slurm_content(settings, additional_commandline + commands)
        if make_slurms:
            new_slurm = output_directory.joinpath(job_id).with_suffix('.slurm')
            new_slurm.write_text(input_slurm)
        result = subprocess.run(['sbatch'], text=True, input=input_slurm, capture_output=True)
        print(result.stdout, result.stderr)
        job_number = re.search(r'\d+', str(result.stdout)).group(0)


    #actual simulation
    settings['-J'] = 'prod' + job_id
    settings['-e'] = output_directory.with_stem(settings['-J'])
    settings['-o'] = output_directory.joinpath(settings['-J'])
    input_slurm = make_slurm_content(settings, additional_commandline + [
        f'{gmxbin} mdrun -cpi {shortname}_prod.cpt -v -deffnm {shortname}_prod'])

    if make_slurms:
        new_slurm = output_directory.joinpath(settings['-J']).with_suffix('.slurm')
        new_slurm.write_text(input_slurm)
    result = subprocess.run(['sbatch'] if gromacs_production_only else ['sbatch', '-d', f'afterok:{job_number}'], text=True, input=input_slurm, capture_output=True)
    print(result.stdout, result.stderr)


def alphafold_analysis(seq_df: ESM.SequenceDataframe, alphafold_directory: str):
    alphafold_dir = Path(alphafold_directory)
    for folder in alphafold_dir.iterdir():
        chi_label = folder.stem
        pdb = folder.joinpath('ranked_0.pdb')
        chi_plddt = Analysis.get_plddt_dict_from_pdb(pdb)[seq_df.get_sequence(chi_label)]
        seq_df.loc[chi_label, 'Overall Stability'] = Analysis.overall_confidence(chi_plddt)
        if len(inheritance_dict := seq_df.get_description(chi_label)) == 2:
            plddt_dict = {}
            for parent in inheritance_dict.keys():
                plddt_dict[parent] = Analysis.get_plddt_dict_from_pdb(
                    alphafold_dir.joinpath(parent, 'ranked_0').with_suffix('.pdb'))[
                    seq_df.get_sequence(parent)]
            plddt_dict[str(chi_label)] = chi_plddt
            relative_stability = Analysis.relative_stabilty(plddt_dict, inheritance_dict)
            seq_df.loc[chi_label, 'Relative Stability (%)'] = relative_stability
    seq_df = ESM.SequenceDataframe(unconverted_df=seq_df.dropna())
    return seq_df


class FastaArguments:
    fasta_toggles: dict
    Create_an_alignment: bool
    parent_msa: str
    number_of_subunits: int
    output_directory: Path
    collective_fasta: str
    sequence_of_interest: str
    base_identifier: str
    sequence_identity_cutoff: float

    def __init__(self, fasta_dict):
        for key, value in fasta_dict.items():
            setattr(self, key, value)


class SubmissionArguments:
    submission_toggles: dict
    create_slurms: bool
    sbatch_slurms: bool
    alphafold_settings: dict = {"-p": "gpu", "--gres": "gpu:h200:1", "-N": 1, "-n": 3, "-A": "enter_here",
                                "--mem": "400000", "--time": "72:00:00", "-J": "enter_slurm_name"},
    embedding_settings: dict = {"-p": "gpu", "--gres": "gpu:a100:1", "-N": 1, "-n": 3, "-A": "enter_here",
                                "--time": "72:00:00", "-J": "enter_slurm_name"}
    gromacs_settings: dict = {"-p": "gpu", "--gres": "gpu:1", "-N": 1, "-A": "", "--time": "72:00:00",
                              "-J": "eidsars_emb", "--ntasks-per-node": "10"}
    gromacs_setup: dict
    custom_label_list: str
    proteins_per_slurm: int
    alphafold_shell_script: str
    embedding_shell_script: str

    def __init__(self, fasta_dict):
        for key, value in fasta_dict.items():
            setattr(self, key, value)


class AnalysisArguments:
    analysis_toggles: dict
    make_plddts: str
    make_pdbs: str
    metadata: dict
    analysis_output_file: str
    column_names: dict
    filename_stems: list
    relative_stability: list
    overall_chimera_stability: list

    def __init__(self, fasta_dict):
        [setattr(self, key, value) for key, value in fasta_dict.items()]


class ChimeraArgs:
    argument_dict: dict
    operation_toggles: dict
    run_fasta_operation: str
    alphafold_submission: str
    run_analysis_operation: str
    fasta_args: FastaArguments
    submission_args: SubmissionArguments
    analysis_args: AnalysisArguments

    def __init__(self, json_file):
        with open(json_file, 'rb') as jfile:
            self.argument_dict = load(jfile)
        self.get_dict_args(FastaArguments, 'fasta_args', 'fasta_arguments')
        self.get_dict_args(SubmissionArguments, 'submission_args', 'submission_arguments')
        self.get_dict_args(AnalysisArguments, 'analysis_args', 'analysis_arguments')
        self.operation_toggles = self.argument_dict['operation_toggles']
        self.fasta_args.output_directory = Path(self.fasta_args.output_directory)
        self.embed_chunk_dir = self.fasta_args.output_directory.joinpath('EmbedChunks')
        self.fasta_chunk_dir = self.fasta_args.output_directory.joinpath('FastaChunks')
        self.alphafold_dir = self.fasta_args.output_directory.joinpath(ALPHAFOLD_FOLDERNAME)
        self.fasta_dir = self.fasta_args.output_directory.joinpath(FASTA_FOLDERNAME)
        self.gmx_dir = self.fasta_args.output_directory.joinpath(GMX_FOLDERNAME)
        if Path(self.fasta_args.collective_fasta).exists():
            fasta_dfs = []
            for chunk_fasta in self.fasta_chunk_dir.iterdir():
                fasta_dfs.append(ESM.SequenceDataframe(chunk_fasta))
            self.collective_df = pd.concat(fasta_dfs)
            self.collective_df = self.collective_df[~self.collective_df.index.duplicated(keep='first')]
            self.collective_df = ESM.SequenceDataframe(unconverted_df=self.collective_df)
            self.has_inheritance = True

    def get_dict_args(self, dict_class, attr_name, arg_key):
        dict_args = self.argument_dict[arg_key]
        setattr(self, attr_name, dict_class(dict_args))

    def fasta_operations(self):
        MIN_SPLICE_PERCENT = .35
        MAX_SPLICE_PERCENT = .65
        chimera_df = ChimeraGenerator.create_combinations_no_aln(self.fasta_args.parent_msa,
                                                                 (MIN_SPLICE_PERCENT, MAX_SPLICE_PERCENT),
                                                                 self.fasta_args.collective_fasta)
        fasta_dir = self.fasta_chunk_dir
        fasta_dir.mkdir(exist_ok=True)
        aln_df = ESM.SequenceDataframe(self.fasta_args.parent_msa)
        parent1, parent2 = aln_df.index
        aln_df.add_value(parent1, 'description', {parent1: None})
        aln_df.add_value(parent2, 'description', {parent2: None})
        chunk_size = 2000
        num_chunks = math.ceil(chimera_df.shape[0] / chunk_size)
        chunks: list[ESM.SequenceDataframe] = [ESM.SequenceDataframe(unconverted_df=chunk) for chunk in
                                               np.array_split(chimera_df, num_chunks)]
        for ind, chunk in enumerate(chunks):
            chunk = ESM.SequenceDataframe(unconverted_df=chunk._append(aln_df))
            chunk.dataframe_to_aln(fasta_dir.joinpath(Path(self.fasta_args.collective_fasta).stem + f'_{str(ind)}.inh'))

    def submission_operations(self):
        submission_toggles = self.submission_args.submission_toggles
        if submission_toggles['get_embeddings']:
            self.embed_chunk_dir.mkdir(exist_ok=True)
            for slurm_index, chunk_fasta in enumerate(self.fasta_chunk_dir.iterdir()):
                if submission_toggles['run_stragglers'] and self.embed_chunk_dir.joinpath(chunk_fasta.stem).with_suffix('.pkl').exists():
                    continue
                settings = dict(self.submission_args.embedding_settings)
                job_id = str(slurm_index) + settings.get('-J', 'embedding')
                settings['-J'] = job_id
                if errorfile := settings.get('-e', None) is not None:
                    settings['-e'] = Path(errorfile).with_stem(str(slurm_index) + errorfile.stem)
                else:
                    settings['-e'] = self.fasta_args.output_directory.joinpath(job_id).with_suffix(".err")
                prost_submission(chunk_fasta.__str__(),
                                 self.embed_chunk_dir.joinpath(chunk_fasta.stem).with_suffix('.pkl').__str__(),
                                 self.fasta_args.output_directory, self.submission_args.embedding_shell_script,
                                 settings, submission_toggles['make_slurm_files'])

        if submission_toggles['run_AF']:
            fasta_dir = self.fasta_dir
            fasta_dir.mkdir(exist_ok=True)
            fastas_to_run = []
            if self.submission_args.custom_label_list:
                seq_dict = AccessiontoAlignment.create_dictionary_from_alignment(self.fasta_args.collective_fasta)
                with open(self.submission_args.custom_label_list, 'r') as run_list:
                    chimeras_to_run = [x.replace('\n', '') for x in run_list.readlines() if x.replace('\n', '')] + list(AccessiontoAlignment.create_dictionary_from_alignment(self.fasta_args.parent_msa).keys())
                for label in chimeras_to_run:
                    fasta_file = fasta_dir.joinpath(label).with_suffix(FASTA_EXT)
                    fastas_to_run.append(fasta_file)
                    AccessiontoAlignment.fasta_creation(fasta_file,
                                                        AccessiontoAlignment.create_seq_records(label, seq_dict[label],
                                                                                                subunit_count=self.fasta_args.number_of_subunits))
            else:
                df = ESM.SequenceDataframe(self.fasta_args.collective_fasta)
                df.make_individual_fasta(fasta_dir, self.fasta_args.number_of_subunits)
                for label in df.index:
                    fasta_file = fasta_dir.joinpath(label).with_suffix(FASTA_EXT)
                    fastas_to_run.append(fasta_file)

            if submission_toggles['run_stragglers'] and fastas_to_run:
                stragglers = []
                for fasta in fastas_to_run:
                    if not self.alphafold_dir.joinpath(Path(fasta).stem, 'ranked_0').with_suffix('.pdb').exists():
                        stragglers.append(fasta)
                fastas_to_run = stragglers

            fastas_to_run = [fasta.__str__() for fasta in fastas_to_run]
            print(len(fastas_to_run))
            if fastas_to_run:
                alphafold_submission(fastas_to_run, self.submission_args.alphafold_shell_script,
                                     self.fasta_args.output_directory.__str__(),
                                     self.submission_args.alphafold_settings,
                                     self.submission_args.proteins_per_slurm, submission_toggles['make_slurm_files'])

        if submission_toggles['run_gromacs']:
            if not Path(self.submission_args.custom_label_list).exists():
                raise FileNotFoundError('Please submit a valid custom_label_list in the argument json.')
            with (open(self.submission_args.custom_label_list, 'r') as run_list):
                gromacs_commands = self.submission_args.gromacs_setup
                self.gmx_dir.mkdir(exist_ok=True)
                gmx_bin = gromacs_commands['gmx_command']
                for label in [x.replace('\n', '') for x in run_list.readlines() if x.replace('\n', '')] + list(AccessiontoAlignment.create_dictionary_from_alignment(self.fasta_args.parent_msa).keys()):
                    label_dir = self.gmx_dir.joinpath(label)
                    label_dir.mkdir(exist_ok=True)
                    pdb = self.alphafold_dir.joinpath(label, 'ranked_0').with_suffix('.pdb')
                    renamed_pdb = pdb.with_stem(label)
                    renamed_pdb.write_bytes(pdb.read_bytes())
                    gmx_submission(renamed_pdb, self.submission_args.gromacs_settings, label_dir.__str__(),
                                   gromacs_commands['working_directory'], gromacs_commands['forcefield'],
                                   submission_toggles['make_slurm_files']
                                   , gmx_bin, gromacs_commands['necessary_commands'])

    def analyze_gromacs(self):
        # get all directories that are not slurm
        rmsf_files = {}
        tpr_xtc_pairs = {}
        for gmx_folder in [direc for direc in self.gmx_dir.iterdir() if direc.is_dir() and direc.stem != 'Slurms']:
            # create trajectory movies
            label = gmx_folder.stem
            tpr_file = gmx_folder.joinpath(label + '_prod.tpr')
            centered_xtc = gmx_folder.joinpath(label + '_center.xtc')
            tpr_xtc_pairs[tpr_file.__str__()] = centered_xtc.__str__()
            GromacsAnalysis.center_xtc(tpr_file, tpr_file.with_suffix('.xtc'), centered_xtc,
                                       self.submission_args.gromacs_setup['necessary_commands'])
            GromacsAnalysis.create_trajectory_movie_pdb(tpr_file, centered_xtc,
                                                        gmx_folder.joinpath(label + '_movie.pdb'),
                                                        self.submission_args.gromacs_setup['necessary_commands'])
            # create rmsf files
            rmsf_file = gmx_folder.joinpath(label.replace("prod", "rmsf") + '.xvg')
            rmsf_files[label] = rmsf_file.__str__()
            GromacsAnalysis.create_rmsf_file(tpr_file, centered_xtc, rmsf_file,
                                             self.submission_args.gromacs_setup['necessary_commands'])
        GromacsAnalysis.graph_rmsd_mdanalysis(tpr_xtc_pairs)
        GromacsAnalysis.graph_rmsf(rmsf_files)

    def analyze_embeddings(self):
        embeddings = []
        func = ESM.NormType.dot_product
        dist_type = func.name
        for chunk_fasta in self.fasta_chunk_dir.iterdir():
            embed_file = self.embed_chunk_dir.joinpath(chunk_fasta.stem).with_suffix('.pkl')
            if not chunk_fasta.exists() and embed_file.exists():
                raise FileNotFoundError("Either the fasta or the corresponding embedding pkl file doesn't exist")
            embedding_df = ESM.SequenceDataframe(chunk_fasta.__str__(), embed_file.__str__())
            embedding_df.score_all_per_res(func)
            embedding_df[f'{dist_type}_rank'] = embedding_df[dist_type].rank(
                ascending=func != ESM.NormType.cosine and func != ESM.NormType.dot_product)
            embedding_df.drop(columns=['aln_sequence', 'sequence', 'description'])
            del embedding_df.EMBEDDINGS_DICT
            embeddings.append(embedding_df)

        embeddings = pd.concat(embeddings)
        embeddings = embeddings.loc[:, ~embeddings.columns.duplicated()]
        embeddings = embeddings[~embeddings.index.duplicated(keep='first')]
        embeddings = ESM.SequenceDataframe(unconverted_df=embeddings)
        embeddings.drop(columns=['aln_sequence'])
        if self.analysis_args.analysis_output_file:
            embeddings.save_df(self.analysis_args.analysis_output_file, self.analysis_args.metadata)

    def analysis_operations(self):
        analysis_toggles = self.analysis_args.analysis_toggles
        if analysis_toggles['analyze_embeddings']:
            self.analyze_embeddings()

        if analysis_toggles['analyze_alphafold']:
            alphafold_df = alphafold_analysis(self.collective_df, self.alphafold_dir.__str__())
            alphafold_df.save_df(self.analysis_args.analysis_output_file, self.analysis_args.metadata)

        if analysis_toggles['analyze_gromacs']:
            self.analyze_gromacs()


class NonHomologyChimeraArgs(ChimeraArgs):
    def __init__(self, nonhomology_json_file):
        super().__init__(nonhomology_json_file)


class HomomerChimeraArgs(ChimeraArgs):

    def __init__(self, arg_json, full_use=True):
        super().__init__(arg_json)
        self.all_fastas = None
        self.base_splice = None
        self.operation_toggles = self.argument_dict['operation_toggles']
        if Path(self.fasta_args.parent_msa).exists():
            self.parent_df = ESM.SequenceDataframe(self.fasta_args.parent_msa)
        elif full_use:
            raise FileNotFoundError(f'Could not find parent msa {self.fasta_args.parent_msa}')

    def make_inheritance(self):
        for label, data in self.parent_df.iterrows():
            self.collective_df.add_protein(label, data['sequence'], {})
            self.collective_df.add_protein(data['chi_file_stem'],
                                           *AccessiontoAlignment.alignment_finder(self.base_splice, label,
                                                                                  self.fasta_args.base_identifier,
                                                                                  self.fasta_args.parent_msa)[:2])
        self.collective_df.dataframe_to_aln(self.fasta_args.collective_fasta)

    def make_fasta_paths(self):
        self.base_splice = AccessiontoAlignment.extract_seq_from_fasta(self.fasta_args.sequence_of_interest)
        self.parent_df['chi_seq'] = self.parent_df.index.map(
            lambda label: AccessiontoAlignment.alignment_finder(self.base_splice, label,
                                                                self.fasta_args.base_identifier,
                                                                self.fasta_args.parent_msa)[0].replace('-', ''))
        self.parent_df['file_stem'] = self.parent_df.index
        self.parent_df['chi_file_stem'] = self.parent_df.index.map()

        fastas = self.parent_df['file_stem'].map(
            lambda stem: self.fasta_dir.joinpath(stem).with_suffix(FASTA_EXT).__str__()).to_list()
        chi_fastas = self.parent_df['chi_file_stem'].map(
            lambda stem: self.fasta_dir.joinpath(stem).with_suffix(FASTA_EXT).__str__()).to_list()
        self.all_fastas = fastas + chi_fastas
        self.make_inheritance()

    def fasta_operations(self):
        self.make_fasta_paths()
        num_of_subunits = self.fasta_args.number_of_subunits
        fasta_dir = self.fasta_args.output_directory.joinpath(FASTA_FOLDERNAME)
        os.makedirs(fasta_dir, exist_ok=True)
        for label, data in self.parent_df.iterrows():
            AccessiontoAlignment.fasta_creation(fasta_dir.joinpath(data['file_stem']).with_suffix(FASTA_EXT),
                                                AccessiontoAlignment.create_seq_records(label, data['sequence'],
                                                                                        subunit_count=num_of_subunits))
            AccessiontoAlignment.fasta_creation(fasta_dir.joinpath(data['chi_file_stem']).with_suffix(FASTA_EXT),
                                                AccessiontoAlignment.create_seq_records(label, data['chi_seq'],
                                                                                        subunit_count=num_of_subunits))

    def analysis_operations(self):
        analysis_toggles = self.analysis_args.analysis_toggles
        if not self.has_inheritance:
            self.make_inheritance()
        if analysis_toggles['analyze_alphafold']:
            alphafold_df = alphafold_analysis(self.collective_df, self.alphafold_dir.__str__())
            alphafold_df.save_df(self.analysis_args.analysis_output_file, self.analysis_args.metadata)


class ShiftedChimeraArgs(ChimeraArgs):
    argument_dict: dict
    operation_toggles: dict
    run_fasta_operation: str
    alphafold_submission: str
    run_analysis_operation: str

    def __init__(self, shifted_json_file):
        super().__init__(shifted_json_file)

    def fasta_operations(self):
        # TODO make splice_percents modular
        min_splice_percent = .35
        max_splice_percent = .65
        seq_dict = AccessiontoAlignment.create_dictionary_from_alignment(self.fasta_args.parent_msa)
        aln_len = len(list(seq_dict.values())[0])
        aln_dfs = []
        #Could improve algorithm by only looking at full sequences instead of looking at gapped sequences
        for scanner in range(round(min_splice_percent * aln_len), round(max_splice_percent * aln_len)):
            aln_dfs.append(
                ChimeraGenerator.create_chimera_combinations(self.fasta_args.parent_msa, scanner, 0))
        chimera_df = pd.concat(aln_dfs)
        chimera_df=chimera_df.loc[~chimera_df.index.duplicated(keep='first'), :]
        self.fasta_chunk_dir.mkdir(exist_ok=True)
        chimera_df.__class__ = ESM.SequenceDataframe
        chimera_df.get_sequence_identity()
        chimera_df=chimera_df[chimera_df['identity']<=self.fasta_args.sequence_identity_cutoff]
        chimera_df.__class__ = ESM.SequenceDataframe
        chimera_df.dataframe_to_aln(self.fasta_args.collective_fasta)
        chimera_df.dataframe_to_multi_fa(Path(self.fasta_args.collective_fasta).with_suffix('.mfa'))
        num_chunks = chimera_df.shape[0] / 2000
        chunks: list[ESM.SequenceDataframe] = [ESM.SequenceDataframe(unconverted_df=chunk) for chunk in
                                               np.array_split(chimera_df, num_chunks)]
        aln_df = ESM.SequenceDataframe(self.fasta_args.parent_msa)
        parent1, parent2 = aln_df.index
        aln_df.add_value(parent1, 'description', {parent1: None})
        aln_df.add_value(parent2, 'description', {parent2: None})
        for ind, chunk in enumerate(chunks):
            chunk = ESM.SequenceDataframe(unconverted_df=chunk._append(aln_df))
            chunk.dataframe_to_aln(
                self.fasta_chunk_dir.joinpath(Path(self.fasta_args.collective_fasta).stem + f'_{str(ind)}.inh'))


    def analysis_operations(self):
        analysis_toggles = self.analysis_args.analysis_toggles
        # TODO make this universal
        if analysis_toggles['analyze_embeddings'] and not analysis_toggles['analyze_alphafold']:
            embeddings = []
            func = ESM.NormType.dot_product
            dist_type = func.name
            for chunk_fasta in self.fasta_chunk_dir.iterdir():
                embed_file = self.embed_chunk_dir.joinpath(chunk_fasta.stem).with_suffix('.pkl')
                if not chunk_fasta.exists() and embed_file.exists():
                    raise FileNotFoundError("Either the fasta or the corresponding embedding pkl file doesn't exist")
                embedding_df = ESM.SequenceDataframe(chunk_fasta.__str__(), embed_file.__str__())
                embedding_df.score_all_per_res(func)
                embedding_df[f'{dist_type}_rank'] = embedding_df[dist_type].rank(
                    ascending=func != ESM.NormType.cosine and func != ESM.NormType.dot_product)
                embedding_df.drop(columns=['aln_sequence', 'sequence', 'description'])
                embedding_df.get_sequence_identity()
                del embedding_df.EMBEDDINGS_DICT
                embeddings.append(embedding_df)

            embeddings = pd.concat(embeddings)
            embeddings = embeddings.loc[:, ~embeddings.columns.duplicated()]
            embeddings = embeddings[~embeddings.index.duplicated(keep='first')]
            embeddings = ESM.SequenceDataframe(unconverted_df=embeddings)
            embeddings.drop(columns=['aln_sequence'])
            if self.analysis_args.analysis_output_file:
                embeddings.save_df(self.analysis_args.analysis_output_file, self.analysis_args.metadata)

        if analysis_toggles['analyze_alphafold']:
            alphafold_df = alphafold_analysis(self.collective_df, self.alphafold_dir.__str__())
            if analysis_toggles['analyze_embeddings']:
                embeddings = []
                temp_dict = {}
                for chunk_fasta in self.fasta_chunk_dir.iterdir():
                    embed_file = self.embed_chunk_dir.joinpath(chunk_fasta.stem).with_suffix('.pkl')
                    if not chunk_fasta.exists() and embed_file.exists():
                        raise FileNotFoundError(
                            "Either the fasta or the corresponding embedding pkl file doesn't exist")
                    embedding_df = ESM.SequenceDataframe(chunk_fasta.__str__(), embed_file.__str__())
                    temp_dict = temp_dict | embedding_df.EMBEDDINGS_DICT
                    embeddings.append(embedding_df)
                embeddings = pd.concat(embeddings)
                embeddings.drop(columns=['aln_sequence'])
                embeddings = embeddings.loc[list(alphafold_df.index) + list(
                    AccessiontoAlignment.create_dictionary_from_alignment(self.fasta_args.parent_msa).keys())]
                embeddings = embeddings.loc[:, ~embeddings.columns.duplicated()]
                embeddings = embeddings[~embeddings.index.duplicated(keep='first')]
                embeddings = ESM.SequenceDataframe(unconverted_df=embeddings)
                embeddings.EMBEDDINGS_DICT = {key: temp_dict[key] for key in list(embeddings.index) + list(
                    AccessiontoAlignment.create_dictionary_from_alignment(self.fasta_args.parent_msa).keys())}
                embeddings.score_all_per_res(ESM.NormType.dot_product)
                embeddings = pd.concat([embeddings, alphafold_df], axis=1)
                embeddings = embeddings.loc[:, ~embeddings.columns.duplicated(keep='first')]
                embeddings = embeddings[~embeddings.index.duplicated(keep='first')]
                alphafold_df = ESM.SequenceDataframe(unconverted_df=embeddings)
            alphafold_df.get_sequence_identity()
            alphafold_df.save_df(self.analysis_args.analysis_output_file, self.analysis_args.metadata)

        if analysis_toggles['analyze_gromacs']:
            self.analyze_gromacs()
